


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r results=FALSE, cache=TRUE}

library('tidyverse')

# the data file uses ';' as delimiter, and for this we use the read_csv2 function
resReviewsData <- read_csv2('yelpRestaurantReviews_Sample.csv')


#Summary of the dataset
summary(resReviewsData)

install.packages("skimr")
library(skimr)
# Generate the summary
skim(resReviewsData)

#Rename the 'starsReview' column to 'stars'
resReviewsData <-  resReviewsData %>% rename(stars=starsReview)


#number of reviews by star-rating
resReviewsData %>% group_by(stars) %>% count()

# calculate correlation between Business Stars and stars
cor(resReviewsData$starsBusiness, resReviewsData$stars)

#distribution of star ratings
hist(resReviewsData$stars)
ggplot(resReviewsData, aes(x= funny, y=stars)) +geom_point()
ggplot(resReviewsData, aes(x= cool, y=stars)) +geom_point()
ggplot(resReviewsData, aes(x= useful, y=stars)) +geom_point()

#Correlation between the average number of funny votes and the star rating
avg_funny <- resReviewsData %>% 
  group_by(stars) %>% 
  summarise(mean_funny = mean(funny))

ggplot(avg_funny, aes(x = stars, y = mean_funny)) +
  geom_line() +
  xlab("Star Rating") +
  ylab("Average Funny Votes") +
  ggtitle("Average Funny Votes vs Star Rating")

#Correlation between the average number of cool votes and the star rating
avg_cool <- resReviewsData %>% 
  group_by(stars) %>% 
  summarise(mean_cool = mean(cool))

ggplot(avg_cool, aes(x = stars, y = mean_cool)) +
  geom_line() +
  xlab("Star Rating") +
  ylab("Average Cool Votes") +
  ggtitle("Average Cool Votes vs Star Rating")

#Correlation between the average number of useful votes and the star rating
avg_useful <- resReviewsData %>% 
  group_by(stars) %>% 
  summarise(mean_useful = mean(useful))

ggplot(avg_useful, aes(x = stars, y = mean_useful)) +
  geom_line() +
  xlab("Star Rating") +
  ylab("Average Useful Votes") +
  ggtitle("Average Useful Votes vs Star Rating")

#The reviews are from various locations 
resReviewsData %>%   group_by(state) %>% tally() %>% view()
 #Can also check the postal_codes`
resReviewsData %>%   group_by(postal_code) %>% tally() %>% view()


#Keep only the those reviews from 5-digit postal-codes  
rrData <- resReviewsData %>% filter(str_detect(postal_code, "^[0-9]{1,5}"))
```

```{r}
# create a new column called 'sentiment' indicating positive or negative
resReviewsData <- resReviewsData %>% 
  mutate(sentiment = ifelse(stars >= 4, "positive", "negative"))

# summarize the data by sentiment
sentiment_summary <- resReviewsData %>% group_by(sentiment) %>% count()

# print the summary table
sentiment_summary

# create a bar chart of sentiment summary
library(ggplot2)

ggplot(sentiment_summary, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Distribution of Positive and Negative Reviews", x = "Sentiment", y = "Number of Reviews") +
  theme_minimal()



```

Using tidytext for tokenization, removing stopwords, stemming/lemmatization, etc.
```{r message=FALSE , cache=TRUE}

install.packages("tidytext")
install.packages("SnowballC")
install.packages("textstem")


library(tidytext)
library(SnowballC)
library(textstem)

#tokenize the text of the reviews in the column named 'text'
rrTokens <- rrData %>% unnest_tokens(word, text)
   # this will retain all other attributes
#Or we can select just the review_id and the text column
rrTokens <- rrData %>% select(review_id, stars, text ) %>% unnest_tokens(word, text)

#How many tokens?
rrTokens %>% distinct(word) %>% dim()


#remove stopwords
rrTokens <- rrTokens %>% anti_join(stop_words)
 #compare with earlier - what fraction of tokens were stopwords?
rrTokens %>% distinct(word) %>% dim()


#count the total occurrences of different words, & sort by most frequent
rrTokens %>% count(word, sort=TRUE) %>% top_n(10)


#Are there some words that occur in a large majority of reviews, or which are there in very few reviews?   Let's remove the words which are not present in at least 10 reviews
rareWords <-rrTokens %>% count(word, sort=TRUE) %>% filter(n<10)
xx<-anti_join(rrTokens, rareWords)

#check the words in xx .... 
xx %>% count(word, sort=TRUE) %>% view()

#Among the least frequently occurring words, are those starting with or including numbers (as in 6oz, 1.15,...).  
#To remove these
xx2<- xx %>% filter(str_detect(word,"[0-9]")==FALSE)
#the variable xx, xx2 are for checking ....if this is what we want, set the rrTokens to the reduced set of words.  And we can remove xx, xx2 from the environment.
rrTokens<- xx2
rm(xx, xx2)

```



Analyze words by star ratings 
```{r  message=FALSE , cache=TRUE}

#Check words by star rating of reviews
rrTokens %>% group_by(stars) %>% count(word, sort=TRUE) %>% arrange(desc(stars)) %>% view()


#proportion of word occurrence by star ratings
ws <- rrTokens %>% group_by(stars) %>% count(word, sort=TRUE)
ws<-  ws %>% group_by(stars) %>% mutate(prop=n/sum(n))

#check the proportion of 'love' among reviews with 1,2,..5 stars 
ws %>% filter(word=='love')

#what are the most commonly used words by star rating
ws %>% group_by(stars) %>% arrange(stars, desc(prop)) %>% view()

#To see the top 20 words by star ratings
ws %>% group_by(stars) %>% arrange(stars, desc(prop)) %>% filter(row_number()<=20) %>% view()

#To plot this
p <- ws %>% group_by(stars) %>% arrange(stars, desc(prop)) %>% filter(row_number()<=20) %>% ggplot(aes(word, prop))+geom_col()+coord_flip()+facet_wrap((~stars))

ggsave("myplot.png", p, width = 30, height = 20, units = "cm", dpi = 300)


#Or, separate plots by stars
ws %>% filter(stars==1)  %>%  ggplot(aes(word, n)) + geom_col()+coord_flip()


#Can we get a sense of which words are related to higher/lower star ratings in general? 
#One approach is to calculate the average star rating associated with each word - can sum the star ratings associated with reviews where each word occurs in.  Can consider the proportion of each word among reviews with a star rating.
xx<- ws %>% group_by(word) %>% summarise(totWS=sum(stars*prop))

#What are the 20 words with highest and lowest star rating
xx %>% top_n(20)
xx %>% top_n(-20)

```
library(textdata)

#take a look at the wordsin the sentimennt dictionaries
get_sentiments("bing") %>% view()
get_sentiments("nrc") %>% view()
get_sentiments("afinn") %>% view()

#3a#sentiment of words in rrTokens
rrSenti_bing1 <- rrTokens %>% inner_join(get_sentiments("bing"), by="word") 
rrSenti_nrc1<- rrTokens %>% inner_join(get_sentiments("nrc"), by="word") 
rrSenti_affin1<- rrTokens %>% inner_join(get_sentiments("afinn"), by="word") 

binn<- nrow(rrSenti_bing1)
nrcn <- nrow(rrSenti_nrc1)
affn <- nrow(rrSenti_affin1)

table_terms <- matrix(c(binn,nrcn,affn),ncol=3,byrow=TRUE)
colnames(table_terms) <- c("Bing","NRC","AFFIN")
rownames(table_terms) <- c("Number of terms")
table_terms <- as.table(table_terms)
barplot(table_terms, main =" Matching terms in each dictionary")

#3b
bing_nrc_overlap <- intersect(rrSenti_bing1$word, rrSenti_nrc1$word)
bing_affin_overlap <- intersect(rrSenti_bing1$word, rrSenti_affin1$word)
nrc_affin_overlap <- intersect(rrSenti_nrc1$word, rrSenti_affin1$word)

# Store overlap counts in variables
bing_nrc_count <- length(bing_nrc_overlap)
bing_affin_count <- length(bing_affin_overlap)
nrc_affin_count <- length(nrc_affin_overlap)

#3c
# Join positive terms with each sentiment dictionary
bing_pos <- inner_join(positive20, get_sentiments("bing"), by = "word")
nrc_pos <- inner_join(positive20, get_sentiments("nrc"), by = "word")
affin_pos <- inner_join(positive20, get_sentiments("afinn"), by = "word")

# Join negative terms with each sentiment dictionary
bing_neg <- inner_join(negative20, get_sentiments("bing"), by = "word")
nrc_neg <- inner_join(negative20, get_sentiments("nrc"), by = "word")
affin_neg <- inner_join(negative20, get_sentiments("afinn"), by = "word")

#Analyze Which words contribute to positive/negative sentiment - we can count the ocurrences of positive/negative sentiment words in the reviews
#Q4
#bing
rrSenti_bing<- rrTokens %>% inner_join(get_sentiments("bing"), by="word") 

#Analyze Which words contribute to positive/negative sentiment - we can count the ocurrences of positive/negative sentiment words in the reviews
xx<-rrSenti_bing %>% group_by(word, sentiment) %>% summarise(totOcc=sum(n)) %>% arrange(sentiment, desc(totOcc))
#negate the counts for the negative sentiment words
xx<- xx %>% mutate (totOcc=ifelse(sentiment=="positive", totOcc, -totOcc))

#the most positive and most negative words
# ungrouping is important because we have grouped by word and sentiment together in the code above
xx<-ungroup(xx)   
top_n(xx, 25) %>% arrange(sentiment, desc(totOcc))
top_n(xx, -25)  %>% arrange(sentiment, desc(totOcc))

#We can plot these
rbind(top_n(xx, 25), top_n(xx, -25)) %>% ggplot(aes(word, totOcc, fill=sentiment)) +geom_col()+coord_flip()

## Ordering of words
orderw <- rbind(top_n(xx, 25), top_n(xx, -25)) %>% mutate(word=reorder(word,totOcc)) 
orderw %>% ggplot(aes(word, totOcc, fill=sentiment)) +geom_col()+coord_flip()

# Review Sentiment Analysis
#So far, we have analyzed overall sentiment across reviews, now let's look into sentiment by review and see how that relates to review's star ratings

#summarise positive/negative sentiment words per review
revSenti_bing <- rrSenti_bing %>% group_by(review_id,stars) %>% summarise(nwords=n(),posSum=sum(sentiment=='positive'), negSum=sum(sentiment=='negative'))

#calculate sentiment score based on proportion of positive, negative words
revSenti_bing<- revSenti_bing %>% mutate(posProp=posSum/nwords, negProp=negSum/nwords)
revSenti_bing<- revSenti_bing %>% mutate(sentiScore=posProp-negProp)

revSenti_bing %>% group_by(stars) %>% summarise(avgPos=mean(posProp), avgNeg=mean(negProp), avgSentiSc=mean(sentiScore))

#considering reviews with 1 & 2 starsReview as negative, and this with 4 & 5 starsReview as positive
revSenti_bing <- revSenti_bing %>% mutate(hiLo=ifelse(stars<=2,-1, ifelse(stars>=4, 1, 0 )))
revSenti_bing <- revSenti_bing %>% mutate(pred_hiLo=ifelse(sentiScore >0, 1, -1))
xx_bing <-revSenti_bing %>% filter(hiLo!=0)
Bing_CM <- table(actual=xx_bing$hiLo, predicted=xx_bing$pred_hiLo )

#calculating the accuracy of the predictions using affin dictionary
#accuracy on training & test data
library(caret)
confusionMatrix(Bing_CM)

#NRC
rrSenti_NRC<- rrTokens %>% inner_join(get_sentiments("nrc"), by="word")

rrSenti_nrc<-rrTokens %>% inner_join(get_sentiments("nrc"), by="word") %>% group_by (word, sentiment) %>% summarise(totOcc=sum(n)) %>% arrange(sentiment, desc(totOcc))

#number of words for the different sentiment categories
rrSenti_nrc %>% group_by(sentiment) %>% summarise(count=n(), sumn=sum(totOcc))

## we have got total 10 sentiments
#top few words for different sentiments
rrSenti_nrc %>% group_by(sentiment) %>% arrange(sentiment, desc(totOcc)) %>% top_n(10) %>% view()

#considering  {anger, disgust, fear sadness, negative} to denote 'bad' reviews, and {positive, joy, anticipation, trust} to denote 'good' reviews
# Geting the GoodBad score for each word
xx1<-rrSenti_nrc %>% mutate(goodBad=ifelse(sentiment %in% c('anger', 'disgust', 'fear', 'sadness', 'negative'), -totOcc, ifelse(sentiment %in% c('positive', 'joy', 'anticipation', 'trust'), totOcc, 0)))

xx1<-ungroup(xx1)
top_n(xx1, 10)
top_n(xx1, -10)

nrcwords <- rbind(top_n(xx1, 25), top_n(xx1, -25)) %>% mutate(word=reorder(word,goodBad)) 
nrcwords %>% ggplot(aes(word, goodBad, fill=goodBad)) +geom_col()+coord_flip()

#Analysis by Review Sentiment
rrSenti_nrc1 <- rrTokens %>% inner_join(get_sentiments("nrc"), by="word") %>% group_by (review_id, stars, sentiment)  %>% summarise(totOcc=sum(n)) %>% arrange(stars, sentiment, desc(totOcc))

# Geting the GoodBad score for each review
xx_nrc <-rrSenti_nrc1 %>% mutate(goodBad=ifelse(sentiment %in% c('anger', 'disgust', 'fear', 'sadness', 'negative'), -totOcc, ifelse(sentiment %in% c('positive', 'joy', 'anticipation', 'trust'), totOcc, 0)))

xx_nrc <-ungroup(xx_nrc)
top_n(xx_nrc, 10)
top_n(xx_nrc, -10)

revSenti_nrc <- xx_nrc %>% group_by(review_id, stars) %>% summarise(nwords=n(),sentiGoodBad =sum(goodBad))
revSenti_nrc %>% group_by(stars) %>% summarise(avgLen=mean(nwords),avgSenti=mean(sentiGoodBad))

#considering reviews with 1 & 2 starsReview as negative, and this with 4 & 5 starsReview as positive
revSenti_nrc <- revSenti_nrc %>% mutate(hiLo=ifelse(stars<=2,-1, ifelse(stars>=4, 1, 0 )))
revSenti_nrc <- revSenti_nrc %>% mutate(pred_hiLo=ifelse(sentiGoodBad >0, 1, -1))
xx_nrc1 <-revSenti_nrc %>% filter(hiLo!=0)
nrc_CM <- table(actual=xx_nrc1$hiLo, predicted=xx_nrc1$pred_hiLo )

#calculating the accuracy of the predictions using affin dictionary
#accuracy on training & test data
confusionMatrix(nrc_CM)

#AFINN
# calculate AFINN sentiment scores for each word
rrSenti_affin1 <- rrTokens %>%
  inner_join(get_sentiments("afinn"), by="word")

# group by word and summarize total occurrences and AFINN scores
xx <- rrSenti_affin1 %>%
  group_by(word) %>%
  summarise(totOcc=sum(value), avgScore=mean(value)) %>%
  arrange(desc(avgScore))

# the most positive and most negative words
xx <- ungroup(xx)
xx %>% top_n(25, avgScore)
xx %>% top_n(-25, avgScore)

# plot the results
rbind(top_n(xx, 25, avgScore), top_n(xx, -25, avgScore)) %>%
  mutate(word=reorder(word, avgScore)) %>%
  ggplot(aes(word, avgScore)) +
  geom_col(aes(fill=avgScore > 0)) +
  scale_fill_manual(values=c("#D73027", "#4575B4")) +
  coord_flip()

rrSenti_afinn<- rrTokens %>% inner_join(get_sentiments("afinn"), by="word")

revSenti_afinn <- rrSenti_afinn %>% group_by(review_id, stars) %>% summarise(nwords=n(), sentiSum =sum(value))

revSenti_afinn %>% group_by(stars) %>% summarise(avgLen=mean(nwords),avgSenti=mean(sentiSum))

#considering reviews with 1 & 2 starsReview as negative, and this with 4 & 5 starsReview as positive
revSenti_afinn <- revSenti_afinn %>% mutate(hiLo=ifelse(stars<=2,-1, ifelse(stars>=4, 1, 0 )))
revSenti_afinn <- revSenti_afinn %>% mutate(pred_hiLo=ifelse(sentiSum >0, 1, -1))
xx<-revSenti_afinn %>% filter(hiLo!=0)
affin_CM <- table(actual=xx$hiLo, predicted=xx$pred_hiLo )

#calculating the accuracy of the predictions using affin dictionary
#accuracy on training & test data
confusionMatrix(affin_CM)


#RF for Bing

#considering only those words which match a sentiment dictionary (for eg.  bing)

#use pivot_wider to convert to a dtm form where each row is for a review and columns correspond to words   
#revDTM_sentiBing <- rrSenti_bing %>%  pivot_wider(id_cols = review_id, names_from = word, values_from = tf_idf)

#Or, since we want to keep the starsReview column
revDTM_sentiBing <- rrSenti_bing %>%  pivot_wider(id_cols = c(review_id,starsReview), names_from = word, values_from = tf_idf)  %>% ungroup()
    #Note the ungroup() at the end -- this is IMPORTANT;  we have grouped based on (review_id, starsReview), and this grouping is retained by default, and can cause problems in the later steps

#filter out the reviews with starsReview=3, and calculate hiLo sentiment 'class'
revDTM_sentiBing <- revDTM_sentiBing %>% filter(starsReview!=3) %>% mutate(hiLo=ifelse(starsReview<=2, -1, 1)) %>% select(-starsReview)

#how many review with 1, -1  'class'
revDTM_sentiBing %>% group_by(hiLo) %>% tally()


#develop a Random forest model to predict hiLo from the words in the reviews

library(ranger)

#replace all the NAs with 0
revDTM_sentiBing<-revDTM_sentiBing %>% replace(., is.na(.), 0)

revDTM_sentiBing$hiLo<- as.factor(revDTM_sentiBing$hiLo)

#Create Dataset of 20,000 records
library(dplyr)
set.seed(1889)
rrsentiBing_10K <- revDTM_sentiBing[sample(nrow(revDTM_sentiBing),20000),]

revDTM_sentiBing <- rrsentiBing_10K

library(rsample)
set.seed(1447)
revDTM_sentiBing_split<- initial_split(revDTM_sentiBing, 0.5)
revDTM_sentiBing_trn<- training(revDTM_sentiBing_split)
revDTM_sentiBing_tst<- testing(revDTM_sentiBing_split)

#Model with Number of trees = 100
rfModel<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiBing_trn %>% select(-review_id), num.trees = 100, importance='permutation', probability = TRUE)

rfModel

#which variables are important
importance(rfModel) %>% view()


#Obtain predictions, and calculate performance
revSentiBing_predTrn<- predict(rfModel, revDTM_sentiBing_trn %>% select(-review_id))$predictions

revSentiBing_predTst<- predict(rfModel, revDTM_sentiBing_tst %>% select(-review_id))$predictions

table(actual=revDTM_sentiBing_trn$hiLo, preds=revSentiBing_predTrn[,2]>0.5)
table(actual=revDTM_sentiBing_tst$hiLo, preds=revSentiBing_predTst[,2]>0.5)

#The optimal threshold from the ROC analyses


library(pROC)
rocTrn <- roc(revDTM_sentiBing_trn$hiLo, revSentiBing_predTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiBing_tst$hiLo, revSentiBing_predTst[,2], levels=c(-1, 1))

plot.roc(rocTrn, col='blue', legacy.axes = TRUE)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')


#Best threshold from ROC analyses
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#Confusion Matrix at bThr for Trn and Tst dataset
table(actual=revDTM_sentiBing_trn$hiLo, preds=revSentiBing_predTrn[,2]>bThr)
table(actual=revDTM_sentiBing_tst$hiLo, preds=revSentiBing_predTst[,2]>bThr)


#Model with Number of trees = 200

rfModel2<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiBing_trn %>% select(-review_id), num.trees = 200, importance='permutation', probability = TRUE)

rfModel2

#which variables are important
#importance(rfModel2) %>% view()


#Obtain predictions, and calculate performance
revSentiBing_predTrn2<- predict(rfModel2, revDTM_sentiBing_trn %>% select(-review_id))$predictions

revSentiBing_predTst2<- predict(rfModel2, revDTM_sentiBing_tst %>% select(-review_id))$predictions

table(actual=revDTM_sentiBing_trn$hiLo, preds=revSentiBing_predTrn2[,2]>0.5)
table(actual=revDTM_sentiBing_tst$hiLo, preds=revSentiBing_predTst2[,2]>0.5)

#The optimal threshold from the ROC analyses


library(pROC)
rocTrn2 <- roc(revDTM_sentiBing_trn$hiLo, revSentiBing_predTrn2[,2], levels=c(-1, 1))
rocTst2 <- roc(revDTM_sentiBing_tst$hiLo, revSentiBing_predTst2[,2], levels=c(-1, 1))

plot.roc(rocTrn2, col='blue', legacy.axes = TRUE)
plot.roc(rocTst2, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')


#Best threshold from ROC analyses
bThr2<-coords(rocTrn2, "best", ret="threshold", transpose = FALSE)
bThr2 <- as.numeric(bThr2)
bThr2

#Confusion Matrix at bThr for Trn and Tst dataset
table(actual=revDTM_sentiBing_trn$hiLo, preds=revSentiBing_predTrn2[,2]>bThr)
table(actual=revDTM_sentiBing_tst$hiLo, preds=revSentiBing_predTst2[,2]>bThr)

#RF for NRC

#remove duplicates from rrSenti_nrc
rrSenti_NRC <-rrSenti_NRC[,-7]
rrSenti_NRC <-rrSenti_NRC[!duplicated(rrSenti_NRC), ]

#Dimensions for rrSenti_nrc 
rrSenti_NRC %>% dim()

#Dimensions for the distinct word tokens in rrSenti_nrc
rrSenti_NRC %>% distinct(word) %>% dim()

#create Document Term Matrix
revDTM_sentiNrc <- rrSenti_NRC %>%  pivot_wider(id_cols = c(review_id,starsReview), names_from = word, values_from = tf_idf)  %>% ungroup()

#Dimensions for revDTM_sentiNrc
revDTM_sentiNrc %>% dim()
view(revDTM_sentiNrc)

#filter out the reviews with starsReview=3, and calculate hiLo sentiment 'class'
revDTM_sentiNrc <- revDTM_sentiNrc %>% filter(starsReview!=3) %>% mutate(hiLo=ifelse(starsReview<=2, -1, 1)) %>% select(-starsReview)

#Dimensions for revDTM_sentiNrc
revDTM_sentiNrc %>% dim()

#replace all the NAs with 0
revDTM_sentiNrc<-revDTM_sentiNrc %>% replace(., is.na(.), 0)

#Convert hiLo from num to factor
revDTM_sentiNrc$hiLo<- as.factor(revDTM_sentiNrc$hiLo)

#how many review with 1, -1  'class'
revDTM_sentiNrc %>% group_by(hiLo) %>% tally()


#develop a Random forest model to predict hiLo from the words in the reviews

#Create Dataset of 20,000 records

set.seed(1889)
rrsentiNrc_10K <- revDTM_sentiNrc[sample(nrow(revDTM_sentiNrc),20000),]

revDTM_sentiNrc <- rrsentiNrc_10K

set.seed(1447)
revDTM_sentiNrc_split<- initial_split(revDTM_sentiNrc, 0.5)
revDTM_sentiNrc_trn<- training(revDTM_sentiNrc_split)
revDTM_sentiNrc_tst<- testing(revDTM_sentiNrc_split)

#Model with Number of trees = 100


rfModel3<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiNrc_trn %>% select(-review_id), num.trees = 100, importance='permutation', probability = TRUE)

rfModel3

#which variables are important


#Obtain predictions, and calculate performance
revSentiNrc_predTrn<- predict(rfModel3, revDTM_sentiNrc_trn %>% select(-review_id))$predictions

revSentiNrc_predTst<- predict(rfModel3, revDTM_sentiNrc_tst %>% select(-review_id))$predictions

table(actual=revDTM_sentiNrc_trn$hiLo, preds=revSentiNrc_predTrn[,2]>0.5)
table(actual=revDTM_sentiNrc_tst$hiLo, preds=revSentiNrc_predTst[,2]>0.5)

#The optimal threshold from the ROC analyses

rocTrn3 <- roc(revDTM_sentiNrc_trn$hiLo, revSentiNrc_predTrn[,2], levels=c(-1, 1))
rocTst3 <- roc(revDTM_sentiNrc_tst$hiLo, revSentiNrc_predTst[,2], levels=c(-1, 1))

plot.roc(rocTrn3, col='blue', legacy.axes = TRUE)
plot.roc(rocTst3, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')


#Best threshold from ROC analyses
bThr3<-coords(rocTrn3, "best", ret="threshold", transpose = FALSE)
bThr3 <- as.numeric(bThr3)
bThr3

#Confusion Matrix at bThr for Trn and Tst dataset
table(actual=revDTM_sentiNrc_trn$hiLo, preds=revSentiNrc_predTrn[,2]>bThr3)
table(actual=revDTM_sentiNrc_tst$hiLo, preds=revSentiNrc_predTst[,2]>bThr3)



#Model with Number of trees = 200

rfModel5<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiNrc_trn %>% select(-review_id), num.trees = 200, importance='permutation', probability = TRUE)

rfModel5


#Obtain predictions, and calculate performance
revSentiNrc_predTrn2<- predict(rfModel5, revDTM_sentiNrc_trn %>% select(-review_id))$predictions

revSentiNrc_predTst2<- predict(rfModel5, revDTM_sentiNrc_tst %>% select(-review_id))$predictions

table(actual=revDTM_sentiNrc_trn$hiLo, preds=revSentiNrc_predTrn2[,2]>0.5)
table(actual=revDTM_sentiNrc_tst$hiLo, preds=revSentiNrc_predTst2[,2]>0.5)

#The optimal threshold from the ROC analyses

rocTrn5 <- roc(revDTM_sentiNrc_trn$hiLo, revSentiNrc_predTrn2[,2], levels=c(-1, 1))
rocTst5 <- roc(revDTM_sentiNrc_tst$hiLo, revSentiNrc_predTst2[,2], levels=c(-1, 1))

plot.roc(rocTrn5, col='blue', legacy.axes = TRUE)
plot.roc(rocTst5, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')


#Best threshold from ROC analyses
bThr5<-coords(rocTrn5, "best", ret="threshold", transpose = FALSE)
bThr5 <- as.numeric(bThr5)
bThr5

#Confusion Matrix at bThr for Trn and Tst dataset
table(actual=revDTM_sentiNrc_trn$hiLo, preds=revSentiNrc_predTrn2[,2]>bThr5)
table(actual=revDTM_sentiNrc_tst$hiLo, preds=revSentiNrc_predTst2[,2]>bThr5)


#RF for AFFIN
#From Afinn Dictionary get the sentiment of words in rrTokens
rrSenti_afinn<- rrTokens %>% inner_join(get_sentiments("afinn"), by="word")

#Dimensions for rrSenti_afinn
rrSenti_afinn %>% dim()

#Dimension for distinct words
rrSenti_afinn %>% distinct(word) %>% dim()

#create Document Term Matrix
revDTM_sentiAfinn <- rrSenti_afinn %>%  pivot_wider(id_cols = c(review_id,starsReview), names_from = word, values_from = tf_idf)  %>% ungroup()

#Dimensions for revDTM_sentiAfinn
revDTM_sentiAfinn %>% dim()

#filter out the reviews with starsReview=3
#calculate hiLo sentiment(1 is assigned to 4 and 5/-1 is assigned to 1 and 2)
revDTM_sentiAfinn <- revDTM_sentiAfinn %>% filter(starsReview!=3) %>% mutate(hiLo=ifelse(starsReview<=2, -1, 1)) %>% select(-starsReview)

#Dimensions for revDTM_sentiAfinn
revDTM_sentiAfinn %>% dim()

#replace all NAs with zero
revDTM_sentiAfinn<-revDTM_sentiAfinn %>% replace(., is.na(.), 0)

#convert hiLo from num to factor
revDTM_sentiAfinn$hiLo<- as.factor(revDTM_sentiAfinn$hiLo)

#no of reviews with 1, -1 class
revDTM_sentiAfinn %>% group_by(hiLo) %>% tally()

#Create Dataset of 20,000 records

set.seed(1889)
rrsentiAFINN_10K <- revDTM_sentiAfinn[sample(nrow(revDTM_sentiAfinn),20000),]
revDTM_sentiAfinn <- rrsentiAFINN_10K

set.seed(1234)

#split the data into training and test dataset (50:50)
revDTM_sentiAfinn_split<- initial_split(revDTM_sentiAfinn, 0.5)
revDTM_sentiAfinn_trn  <- training(revDTM_sentiAfinn_split)
revDTM_sentiAfinn_tst  <- testing(revDTM_sentiAfinn_split)


#Random Forest Model with Number of trees = 100

rfModel1<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiAfinn_trn %>% select(-review_id), num.trees = 100, importance='permutation', probability = TRUE)

#Make predictions from the model on trn and test dataset
revSentiAfinn_predTrn<- predict(rfModel1, revDTM_sentiAfinn_trn %>% select(-review_id))
revSentiAfinn_predTst<- predict(rfModel1, revDTM_sentiAfinn_tst %>% select(-review_id))

#Confusion Matrix at 0.5 for Trn and Tst dataset
table(actual=revDTM_sentiAfinn_trn$hiLo, preds=revSentiAfinn_predTrn$predictions[,2]>0.5)
table(actual=revDTM_sentiAfinn_tst$hiLo, preds=revSentiAfinn_predTst$predictions[,2]>0.5)

#find the optimal TH
rocTrn <- roc(revDTM_sentiAfinn_trn$hiLo, revSentiAfinn_predTrn$predictions[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiAfinn_tst$hiLo, revSentiAfinn_predTst$predictions[,2], levels=c(-1, 1))

plot.roc(rocTrn, col='blue', legacy.axes = TRUE)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#best threshold from ROC
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#Confusion Matrix at bThr for Trn and Tst dataset
table(actual=revDTM_sentiAfinn_trn$hiLo,preds=revSentiAfinn_predTrn$predictions[,2]>bThr)
table(actual=revDTM_sentiAfinn_tst$hiLo, preds=revSentiAfinn_predTst$predictions[,2]>bThr)

#Random Forest Model with Number of trees = 200

rfModel3<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiAfinn_trn %>% select(-review_id), num.trees = 200, importance='permutation', probability = TRUE)

#Make predictions from the model on trn and test dataset
revSentiAfinn_predTrn3<- predict(rfModel3, revDTM_sentiAfinn_trn %>% select(-review_id))
revSentiAfinn_predTst3<- predict(rfModel3, revDTM_sentiAfinn_tst %>% select(-review_id))

#Confusion Matrix at 0.5 for Trn and Tst dataset
table(actual=revDTM_sentiAfinn_trn$hiLo, preds=revSentiAfinn_predTrn3$predictions[,2]>0.5)
table(actual=revDTM_sentiAfinn_tst$hiLo, preds=revSentiAfinn_predTst3$predictions[,2]>0.5)

#find the optimal TH
rocTrn3 <- roc(revDTM_sentiAfinn_trn$hiLo, revSentiAfinn_predTrn3$predictions[,2], levels=c(-1, 1))
rocTst3 <- roc(revDTM_sentiAfinn_tst$hiLo, revSentiAfinn_predTst3$predictions[,2], levels=c(-1, 1))

plot.roc(rocTrn3, col='blue', legacy.axes = TRUE)
plot.roc(rocTst3, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#best threshold from ROC
bThr3<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr3 <- as.numeric(bThr)
bThr3

#Confusion Matrix at bThr for Trn and Tst dataset
table(actual=revDTM_sentiAfinn_trn$hiLo,preds=revSentiAfinn_predTrn3$predictions[,2]>bThr3)
table

###RF for combined
#For combining the matched words of all the three dictionaries change the column 'value' to 'sentiment' in Afinn Dictionary
names(rrSenti_afinn)[names(rrSenti_afinn) == "value"] <- "sentiment"

#Dimensions for matched words from all three dictionaries
rrSenti_bing %>% dim()
rrSenti_NRC %>% dim()
rrSenti_afinn %>% dim()

#Converting the sentiment variable in AFINN dictionary to character
rrSenti_afinn <- rrSenti_afinn %>% mutate(sentiment = as.character(sentiment))

#combine matched words from the three dictionaries

rrSenti_ComboDict <- rbind(rrSenti_bing, rrSenti_NRC, rrSenti_afinn)

#Dimensions for combined set of matched words from dictionaries
rrSenti_ComboDict %>% dim()

#Dimensions for the distinct word tokens in rrSenti_ComboDict
rrSenti_ComboDict %>% distinct(word) %>% dim()

#remove duplicates from rrSenti_ComboDict
rrSenti_ComboDict <-rrSenti_ComboDict[,-8]
rrSenti_ComboDict <-rrSenti_ComboDict[!duplicated(rrSenti_ComboDict), ]

#Dimensions for rrSenti_combo 
rrSenti_ComboDict %>% dim()

#Dimensions for the distinct word tokens in rrSenti_ComboDict
rrSenti_ComboDict %>% distinct(word) %>% dim()

#create Document Term Matrix
revDTM_sentiComboDict <- rrSenti_ComboDict %>%  pivot_wider(id_cols = c(review_id,starsReview), names_from = word, values_from = tf_idf)  %>% ungroup()

#Dimensions for revDTM_sentiComboDict
revDTM_sentiComboDict %>% dim()

#filter out the reviews with starsReview=3
#calculate hiLo sentiment(1 is assigned to 4 and 5/-1 is assigned to 1 and 2)
revDTM_sentiComboDict <- revDTM_sentiComboDict %>% filter(starsReview!=3) %>% mutate(hiLo=ifelse(starsReview<=2, -1, 1)) %>% select(-starsReview)

#Dimensions for revDTM_sentiComboDict
revDTM_sentiComboDict %>% dim()

#replace all NAs with zero
revDTM_sentiComboDict<-revDTM_sentiComboDict %>% replace(., is.na(.), 0)


#convert hiLo from num to factor
revDTM_sentiComboDict$hiLo<- as.factor(revDTM_sentiComboDict$hiLo)

#no of reviews with 1, -1 class
revDTM_sentiComboDict %>% group_by(hiLo) %>% tally()

#Create Dataset of 20,000 records

set.seed(1889)
rrsentiComboDict_10K <- revDTM_sentiComboDict[sample(nrow(revDTM_sentiComboDict),20000),]
revDTM_sentiComboDict <- rrsentiComboDict_10K

set.seed(1234)

#split the data into training and test dataset (50:50)
revDTM_sentiComboDict<- initial_split(revDTM_sentiComboDict, 0.5)
revDTM_sentiComboDict_trn  <- training(revDTM_sentiComboDict)
revDTM_sentiComboDict_tst  <- testing(revDTM_sentiComboDict)

#Random Forest Model with Number of trees = 100



rfModel1<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiComboDict_trn %>% select(-review_id), num.trees = 100, importance='permutation', probability = TRUE)

rfModel1

#Make predictions from the model on trn and test dataset
revDTM_sentiComboDict_trn_predTrn<- predict(rfModel1, revDTM_sentiComboDict_trn %>% select(-review_id))
revDTM_sentiComboDict_tst_predTst<- predict(rfModel1, revDTM_sentiComboDict_tst %>% select(-review_id))

#Confusion Matrix at 0.5 for Trn and Tst dataset
table(actual=revDTM_sentiComboDict_trn$hiLo,preds=revDTM_sentiComboDict_trn_predTrn$predictions[,2]>0.5)
table(actual=revDTM_sentiComboDict_tst$hiLo,preds=revDTM_sentiComboDict_tst_predTst$predictions[,2]>0.5)

#find the optimal TH
rocTrn <- roc(revDTM_sentiComboDict_trn$hiLo,revDTM_sentiComboDict_trn_predTrn$predictions[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiComboDict_tst$hiLo,revDTM_sentiComboDict_tst_predTst$predictions[,2], levels=c(-1, 1))

plot.roc(rocTrn, col='blue', legacy.axes = TRUE)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#best threshold from ROC
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#Confusion Matrix at bThr for Trn and Tst dataset
table(actual=revDTM_sentiComboDict_trn$hiLo,preds=revDTM_sentiComboDict_trn_predTrn$predictions[,2]>bThr)
table(actual=revDTM_sentiComboDict_tst$hiLo,preds=revDTM_sentiComboDict_tst_predTst$predictions[,2]>bThr)
#Random Forest Model with Number of trees = 200

rfModel3<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiComboDict_trn %>% select(-review_id), num.trees = 200, importance='permutation', probability = TRUE)

rfModel3

#Make predictions from the model on trn and test dataset
revDTM_sentiComboDict_trn_predTrn3<- predict(rfModel3, revDTM_sentiComboDict_trn %>% select(-review_id))
revDTM_sentiComboDict_tst_predTst3<- predict(rfModel3, revDTM_sentiComboDict_tst %>% select(-review_id))

#Confusion Matrix at 0.5 for Trn and Tst dataset
table(actual=revDTM_sentiComboDict_trn$hiLo,preds=revDTM_sentiComboDict_trn_predTrn3$predictions[,2]>0.5)
table(actual=revDTM_sentiComboDict_tst$hiLo,preds=revDTM_sentiComboDict_tst_predTst3$predictions[,2]>0.5)

#find the optimal TH
rocTrn3 <- roc(revDTM_sentiComboDict_trn$hiLo,revDTM_sentiComboDict_trn_predTrn3$predictions[,2], levels=c(-1, 1))
rocTst3 <- roc(revDTM_sentiComboDict_tst$hiLo,revDTM_sentiComboDict_tst_predTst3$predictions[,2], levels=c(-1, 1))

plot.roc(rocTrn3, col='blue', legacy.axes = TRUE)
plot.roc(rocTst3, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#best threshold from ROC
bThr3<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr3 <- as.numeric(bThr3)
bThr3

#Confusion Matrix at bThr for Trn and Tst dataset
table(actual=revDTM_sentiComboDict_trn$hiLo,preds=revDTM_sentiComboDict_trn_predTrn3$predictions[,2]>bThr3)
table(actual=revDTM_sentiComboDict_tst$hiLo,preds=revDTM_sentiComboDict_tst_predTst3$predictions[,2]>bThr3)

# Naive Bayes Model with Bing Dictionary with 50:50 split without smoothing
```{r}

nbModel1 <- naiveBayes(hiLo ~ ., data=revDTM_sentiBing_trn %>% select(-review_id))

revSentiBing_NBpredTrn<-predict(nbModel1, revDTM_sentiBing_trn, type = "raw")
revSentiBing_NBpredTst<-predict(nbModel1, revDTM_sentiBing_tst, type = "raw")

# Confusion Matrix considering 0.5 as threshold
table(actual= revDTM_sentiBing_trn$hiLo, predicted= revSentiBing_NBpredTrn[,2]>0.5)
table(actual= revDTM_sentiBing_tst$hiLo, predicted= revSentiBing_NBpredTst[,2]>0.5)

auc(as.numeric(revDTM_sentiBing_trn$hiLo), revSentiBing_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiBing_tst$hiLo), revSentiBing_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiBing_trn$hiLo, revSentiBing_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiBing_tst$hiLo, revSentiBing_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with Bing Dictionary(0.5 Threshold)",)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')


#Best threshold from ROC analyses
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#Confusion Matrix at bThr for Trn and Tst dataset
#Accuracy of Training & Test Data sets
a1 <- table(pred = revSentiBing_NBpredTrn[,2]>bThr, true=revDTM_sentiBing_trn$hiLo)
#mean(predict(nbModel3, revDTM_sentiBing_trn, type='class') == revDTM_sentiBing_trn$hiLo)

a2 <- table(pred = revSentiBing_NBpredTst[,2]>bThr, true=revDTM_sentiBing_tst$hiLo)
#mean(predict(nbModel3, revDTM_sentiBing_tst, type='class') == revDTM_sentiBing_tst$hiLo)

auc(as.numeric(revDTM_sentiBing_trn$hiLo), revSentiBing_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiBing_tst$hiLo), revSentiBing_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiBing_trn$hiLo, revSentiBing_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiBing_tst$hiLo, revSentiBing_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with Bing Dictionary(Best Threshold)",)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

# Naive Bayes Model with Bing Dictionary with 50:50 split with smoothing
```{r}

nbModel11 <- naiveBayes(hiLo ~ ., data=revDTM_sentiBing_trn %>% select(-review_id), laplace = 1)

revSentiBing_NBpredTrn1<-predict(nbModel11, revDTM_sentiBing_trn, type = "raw")
revSentiBing_NBpredTst1<-predict(nbModel11, revDTM_sentiBing_tst, type = "raw")

# Confusion Matrix considering 0.5 as threshold
table(actual= revDTM_sentiBing_trn$hiLo, predicted= revSentiBing_NBpredTrn1[,2]>0.5)
table(actual= revDTM_sentiBing_tst$hiLo, predicted= revSentiBing_NBpredTst1[,2]>0.5)

auc(as.numeric(revDTM_sentiBing_trn$hiLo), revSentiBing_NBpredTrn1[,2])
auc(as.numeric(revDTM_sentiBing_tst$hiLo), revSentiBing_NBpredTst1[,2])

rocTrn <- roc(revDTM_sentiBing_trn$hiLo, revSentiBing_NBpredTrn1[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiBing_tst$hiLo, revSentiBing_NBpredTst1[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with smoothing with Bing Dictionary(0.5 Threshold)",)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')


#Best threshold from ROC analyses
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#Confusion Matrix at bThr for Trn and Tst dataset
#Accuracy of Training & Test Data sets
table(pred = revSentiBing_NBpredTrn[,2]>bThr, true=revDTM_sentiBing_trn$hiLo)
#mean(predict(nbModel3, revDTM_sentiBing_trn, type='class') == revDTM_sentiBing_trn$hiLo)

table(pred = revSentiBing_NBpredTst[,2]>bThr, true=revDTM_sentiBing_tst$hiLo)
#mean(predict(nbModel3, revDTM_sentiBing_tst, type='class') == revDTM_sentiBing_tst$hiLo)

auc(as.numeric(revDTM_sentiBing_trn$hiLo), revSentiBing_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiBing_tst$hiLo), revSentiBing_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiBing_trn$hiLo, revSentiBing_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiBing_tst$hiLo, revSentiBing_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with smoothing  with Bing Dictionary(Best Threshold)",)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

# Naive Bayes Model with NRC Dictionary with 50:50 split
```{r}

nbModel2 <- naiveBayes(hiLo ~ ., data=revDTM_sentiNrc_trn %>% select(-review_id))

revSentiNrc_NBpredTrn<-predict(nbModel2, revDTM_sentiNrc_trn, type = "raw")
revSentiNrc_NBpredTst<-predict(nbModel2, revDTM_sentiNrc_tst, type = "raw")

# Confusion Matrix considering 0.5 as threshold
table(actual= revDTM_sentiNrc_trn$hiLo, predicted= revSentiNrc_NBpredTrn[,2]>0.5)
table(actual= revDTM_sentiNrc_tst$hiLo, predicted= revSentiNrc_NBpredTst[,2]>0.5)

auc(as.numeric(revDTM_sentiNrc_trn$hiLo), revSentiNrc_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiNrc_tst$hiLo), revSentiNrc_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiNrc_trn$hiLo, revSentiNrc_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiNrc_tst$hiLo, revSentiNrc_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with NRC Dictionary",)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#Best threshold from ROC analyses
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#Confusion Matrix at bThr for Trn and Tst dataset
#Accuracy of Training & test data sets
table(pred = revSentiNrc_NBpredTrn[,2]>bThr, true=revDTM_sentiNrc_trn$hiLo)
#mean(predict(nbModel3, revDTM_sentiNrc_trn, type='class') == revDTM_sentiNrc_trn$hiLo)

table(pred = revSentiNrc_NBpredTst[,2]>bThr, true=revDTM_sentiNrc_tst$hiLo)
#mean(predict(nbModel3, revDTM_sentiNrc_tst, type='class') == revDTM_sentiNrc_tst$hiLo)

auc(as.numeric(revDTM_sentiNrc_trn$hiLo), revSentiNrc_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiBing_tst$hiLo), revSentiBing_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiNrc_trn$hiLo, revSentiNrc_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiNrc_tst$hiLo, revSentiNrc_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with smoothing  with Bing Dictionary(Best Threshold)",)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```
# Naive Bayes Model with NRC Dictionary with 50:50 split With Smoothing
```{r}

nbModel2 <- naiveBayes(hiLo ~ ., data=revDTM_sentiNrc_trn %>% select(-review_id), laplace = 1)

revSentiNrc_NBpredTrn<-predict(nbModel2, revDTM_sentiNrc_trn, type = "raw")
revSentiNrc_NBpredTst<-predict(nbModel2, revDTM_sentiNrc_tst, type = "raw")

# Confusion Matrix considering 0.5 as threshold
table(actual= revDTM_sentiNrc_trn$hiLo, predicted= revSentiNrc_NBpredTrn[,2]>0.5)
table(actual= revDTM_sentiNrc_tst$hiLo, predicted= revSentiNrc_NBpredTst[,2]>0.5)

auc(as.numeric(revDTM_sentiNrc_trn$hiLo), revSentiNrc_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiNrc_tst$hiLo), revSentiNrc_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiNrc_trn$hiLo, revSentiNrc_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiNrc_tst$hiLo, revSentiNrc_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with smoothing with NRC Dictionary(0.5 Threshold)",)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#Best threshold from ROC analyses
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#Confusion Matrix at bThr for Trn and Tst dataset
#accuracy on training & test data - Best Threshold
table(pred = revSentiNrc_NBpredTrn[,2]>bThr, true=revDTM_sentiNrc_trn$hiLo)
#mean(predict(nbModel3, revDTM_sentiNrc_trn, type='class') == revDTM_sentiNrc_trn$hiLo)

table(pred = revSentiNrc_NBpredTst[,2]>bThr, true=revDTM_sentiNrc_tst$hiLo)
#mean(predict(nbModel3, revDTM_sentiNrc_tst, type='class') == revDTM_sentiNrc_tst$hiLo)

auc(as.numeric(revDTM_sentiNrc_trn$hiLo), revSentiNrc_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiNrc_tst$hiLo), revSentiNrc_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiNrc_trn$hiLo, revSentiNrc_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiNrc_tst$hiLo, revSentiNrc_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with smoothing  with NRC Dictionary(Best Threshold)",)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

# Naive Bayes Model with Affin Dictionary with 50:50 split
```{r, error=TRUE}

nbModel3 <- naiveBayes(hiLo ~ ., data=revDTM_sentiAfinn_trn %>% select(-review_id))

revSentiAfinn_NBpredTrn<-predict(nbModel3, revDTM_sentiAfinn_trn, type = "raw")
revSentiAfinn_NBpredTst<-predict(nbModel2, revDTM_sentiAfinn_tst, type = "raw")

# Confusion Matrix considering 0.5 as threshold
table(actual= revDTM_sentiAfinn_trn$hiLo, predicted= revSentiAfinn_NBpredTrn[,2]>0.5)
table(actual= revDTM_sentiAfinn_tst$hiLo, predicted= revSentiAfinn_NBpredTst[,2]>0.5)

auc(as.numeric(revDTM_sentiAfinn_trn$hiLo), revSentiAfinn_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiAfinn_tst$hiLo), revSentiAfinn_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiAfinn_trn$hiLo, revSentiAfinn_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiAfinn_tst$hiLo, revSentiAfinn_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with Affin Dictionary(0.5 Threshold)")
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#best threshold from ROC
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#accuracy on training & test data - Best Threshold
table(pred = revSentiAfinn_NBpredTrn[,2]>bThr, true=revDTM_sentiAfinn_trn$hiLo)
#mean(predict(nbModel3, revDTM_sentiAfinn_trn, type='class') == revDTM_sentiAfinn_trn$hiLo)

table(pred = revSentiAfinn_NBpredTst[,2]>bThr, true=revDTM_sentiAfinn_tst$hiLo)
#mean(predict(nbModel3, revDTM_sentiAfinn_tst, type='class') == revDTM_sentiAfinn_tst$hiLo)

auc(as.numeric(revDTM_sentiAfinn_trn$hiLo), revSentiAfinn_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiAfinn_tst$hiLo), revSentiAfinn_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiAfinn_trn$hiLo, revSentiAfinn_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiAfinn_tst$hiLo, revSentiAfinn_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with Affin Dictionary(Best Threshold)")
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```
#Naive Bayes Model with Affin Dictionary with 50:50 split with Smoothing
```{r}

set.seed(1888)
rrsentiAFINN_20K <- revDTM_sentiAfinn[sample(nrow(revDTM_sentiAfinn),20000),]
revDTM_sentiAfinn <- rrsentiAFINN_20K

set.seed(1234)

#split the data into training and test dataset (50:50)
revDTM_sentiAfinn_split<- initial_split(revDTM_sentiAfinn, 0.5)
revDTM_sentiAfinn_trn  <- training(revDTM_sentiAfinn_split)
revDTM_sentiAfinn_tst  <- testing(revDTM_sentiAfinn_split)

nbModel3 <- naiveBayes(hiLo ~ ., data=revDTM_sentiAfinn_trn %>% select(-review_id), laplace = 1)

revSentiAfinn_NBpredTrn<-predict(nbModel3, revDTM_sentiAfinn_trn, type = "raw")
revSentiAfinn_NBpredTst<-predict(nbModel2, revDTM_sentiAfinn_tst, type = "raw")

# Confusion Matrix considering 0.5 as threshold
table(actual= revDTM_sentiAfinn_trn$hiLo, predicted= revSentiAfinn_NBpredTrn[,2]>0.5)
table(actual= revDTM_sentiAfinn_tst$hiLo, predicted= revSentiAfinn_NBpredTst[,2]>0.5)

auc(as.numeric(revDTM_sentiAfinn_trn$hiLo), revSentiAfinn_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiAfinn_tst$hiLo), revSentiAfinn_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiAfinn_trn$hiLo, revSentiAfinn_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiAfinn_tst$hiLo, revSentiAfinn_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with smoothing with Affin Dictionary (0.5 Threshold)")
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#best threshold from ROC
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#accuracy on training & test data - Best Threshold
table(pred = revSentiAfinn_NBpredTrn[,2]>bThr, true=revDTM_sentiAfinn_trn$hiLo)
#mean(predict(nbModel3, revDTM_sentiAfinn_trn, type='class') == revDTM_sentiAfinn_trn$hiLo)

table(pred = revSentiAfinn_NBpredTst[,2]>bThr, true=revDTM_sentiAfinn_tst$hiLo)
#mean(predict(nbModel3, revDTM_sentiAfinn_tst, type='class') == revDTM_sentiAfinn_tst$hiLo)

auc(as.numeric(revDTM_sentiAfinn_trn$hiLo), revSentiAfinn_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiAfinn_tst$hiLo), revSentiAfinn_NBpredTst[,2])

rocTrn <- roc(revDTM_sentiAfinn_trn$hiLo, revSentiAfinn_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiAfinn_tst$hiLo, revSentiAfinn_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with smoothing with Affin Dictionary(Best Threshold)")
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

#Combining all the three dictionaries(bing,Nrc,AFINN) with smoothing in Naive Bayes Model with 50:50 Split
```{r}
## Combining all dictionaries such that the sentiment value is in 8th Column for all and we use that for analysis
# Taking the Combined Dictionary Variables same as defined in the Random Forest
#Create Dataset of 20,000 records
#For combining the matched words of all the three dictionaries change the column 'value' to 'sentiment' in Afinn

#Naive Bayes Model with Smoothing

nbModelAll <- naiveBayes(hiLo ~ ., data=revDTM_sentiComboDict_trn %>% select(-review_id), laplace = 1)

#Make predictions from the model on trn and test dataset
revDTM_sentiComboDict_trn_predTrnNB<- predict(nbModelAll, revDTM_sentiComboDict_trn %>% select(-review_id))
revDTM_sentiComboDict_tst_predTstNB<- predict(nbModelAll, revDTM_sentiComboDict_tst %>% select(-review_id))

#Confusion Matrix at 0.5 for Trn and Tst dataset
NB1 <- table(actual=revDTM_sentiComboDict_trn$hiLo,preds=revDTM_sentiComboDict_trn_predTrnNB)
NB2 <- table(actual=revDTM_sentiComboDict_tst$hiLo,preds=revDTM_sentiComboDict_tst_predTstNB)

confusionMatrix(NB1)
confusionMatrix(NB2)

#auc(as.numeric(revDTM_sentiComboDict_trn$hiLo), revDTM_sentiComboDict_trn_predTrnNB[,2])
#auc(as.numeric(revDTM_sentiComboDict_tst$hiLo), revDTM_sentiComboDict_tst_predTstNB[,2])

#rocTrn <- roc(revDTM_sentiComboDict_trn$hiLo, revDTM_sentiComboDict_trn_predTrnNB$predictions[,2], levels=c(-1, 1))
#rocTst <- roc(revDTM_sentiComboDict_tst$hiLo, revDTM_sentiComboDict_tst_predTstNB$predictions[,2], levels=c(-1, 1))
# plot.roc(rocTrn, col='blue', legacy.axes = TRUE, main = "ROC Plot for Naive Bayes with smoothing with Affin Dictionary(Best Threshold)")
# plot.roc(rocTst, col='red', add=TRUE)
# legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
# 
# #Confusion Matrix at 0.5 for Trn and Tst dataset
# table(actual=revDTM_sentiComboDict_trn$hiLo,preds=revDTM_sentiComboDict_trn_predTrn3$predictions[,2]>0.5)
# table(actual=revDTM_sentiComboDict_tst$hiLo,preds=revDTM_sentiComboDict_tst_predTst3$predictions[,2]>0.5)

```

#Support Vector Machine Models using individual and combined three dictionaries
```{r}

#we learn a model to predict hiLo ratings, from words in reviews
#considering only those words which match a sentiment dictionary (for eg.  bing)

##SVM classification – for restaurant reviews with BING DICTIONARY

##develop a SVM model on the sentiment dictionary terms

svmM1 <- svm(as.factor(hiLo) ~., data = revDTM_sentiBing_trn %>%select(-review_id), kernel="radial", cost=1, scale=FALSE) 
#scale is set to TRUE by default. Since all vars are in tfidf, we shud set scale=FALSE

#Predictions 1
revDTM_predTrn_svm1<-predict(svmM1, revDTM_sentiBing_trn)
revDTM_predTst_svm1<-predict(svmM1, revDTM_sentiBing_tst)
table(actual= revDTM_sentiBing_trn$hiLo, predicted= revDTM_predTrn_svm1)

#Predictions 2
# try different parameters -- rbf kernel gamma, and cost

system.time( svmM2 <- svm(as.factor(hiLo) ~., data = revDTM_sentiBing_trn%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE) )

#Confusion Matrix

revDTM_predTrn_svm2<-predict(svmM2, revDTM_sentiBing_trn)
table(actual= revDTM_sentiBing_trn$hiLo, predicted= revDTM_predTrn_svm2)
revDTM_predTst_svm2<-predict(svmM2, revDTM_sentiBing_tst)
table(actual= revDTM_sentiBing_tst$hiLo, predicted= revDTM_predTst_svm2)

#use the tune function to do a grid search over a set of parameter values : Parameter Tuning

system.time( svm_tune <- tune(svm, as.factor(hiLo) ~., data = revDTM_sentiBing_trn %>% select(-review_id),kernel="radial", ranges = list( cost=c(0.1,1,10,50), gamma = c(0.5,1,2,5, 10),scale=FALSE)) )

#Check performance for different tuned parameters
svm_tune$performances

#Best model
svm_tune$best.parameters
svm_tune$best.model

#predictions from best model : Confusion Matrix

revDTM_predTrn_svm_Best<-predict(svm_tune$best.model, revDTM_sentiBing_trn)
table(actual= revDTM_sentiBing_trn$hiLo, predicted= revDTM_predTrn_svm_Best)
revDTM_predTst_svm_best<-predict(svm_tune$best.model, revDTM_sentiBing_tst)
table(actual= revDTM_sentiBing_tst$hiLo, predicted= revDTM_predTst_svm_best)

# SVM model with NRC Dictionary

#NRC Dictionary

#svmM1_NRC <- svm(as.factor(hiLo) ~., data = revDTM_sentiNrc_trn %>%select(-review_id), kernel="radial", cost=1, scale=FALSE) 
#scale is set to TRUE by default. Since all vars are in tfidf, we shud set scale=FALSE

#Predictions 1
#revDTM_predTrn_svm1_NRC<-predict(svmM1_NRC, revDTM_sentiNrc_trn)
#revDTM_predTst_svm1_NRC<-predict(svmM1_NRC, revDTM_sentiNrc_tst)
#table(actual= revDTM_sentiNrc_trn$hiLo, predicted= revDTM_predTrn_svm1_NRC)

#Predictions 2
# try different parameters -- rbf kernel gamma, and cost

system.time( svmM2_Nrc <- svm(as.factor(hiLo) ~., data = revDTM_sentiNrc_trn%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE) )

#Confusion Matrix

revDTM_predTrn_svm2Nrc<-predict(svmM2_Nrc, revDTM_sentiNrc_trn)
table(actual= revDTM_sentiNrc_trn$hiLo, predicted= revDTM_predTrn_svm2Nrc)
revDTM_predTst_svm2Nrc<-predict(svmM2_Nrc, revDTM_sentiNrc_tst)
table(actual= revDTM_sentiNrc_tst$hiLo, predicted= revDTM_predTst_svm2Nrc)

#use the tune function to do a grid search over a set of parameter values : Parameter Tuning

system.time( svm_tuneNrc <- tune(svm, as.factor(hiLo) ~., data = revDTM_sentiNrc_trn %>% select(-review_id),kernel="radial", ranges = list(cost=c(0.1,1,10,50), gamma = c(0.5,1,2,5, 10),scale=FALSE)) )

#Check performance for different tuned parameters
svm_tuneNrc$performances

#Best model
svm_tuneNrc$best.parameters
svm_tuneNrc$best.model

#predictions from best model : Confusion Matrix

revDTM_predTrn_svmNrc_Best<-predict(svm_tuneNrc$best.model, revDTM_sentiNrc_trn)
table(actual= revDTM_sentiNrc_trn$hiLo, predicted= revDTM_predTrn_svmNrc_Best)
revDTM_predTst_svmNrc_best<-predict(svm_tuneNrc$best.model, revDTM_sentiNrc_tst)
table(actual= revDTM_sentiNrc_tst$hiLo, predicted= revDTM_predTst_svmNrc_best)


###SVM model with AFINN Dictionary

# try different parameters -- rbf kernel gamma, and cost

system.time( svmM2_Afinn <- svm(as.factor(hiLo) ~., data = revDTM_sentiAfinn_trn%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE) )

#Confusion Matrix

revDTM_predTrn_svm2Afinn<-predict(svmM2_Afinn, revDTM_sentiAfinn_trn)
table(actual= revDTM_sentiAfinn_trn$hiLo, predicted= revDTM_predTrn_svm2Afinn)
revDTM_predTst_svm2Afinn<-predict(svmM2_Afinn, revDTM_sentiAfinn_tst)
table(actual= revDTM_sentiAfinn_tst$hiLo, predicted= revDTM_predTst_svm2Afinn)

#use the tune function to do a grid search over a set of parameter values : Parameter Tuning

system.time( svm_tuneAfinn <- tune(svm, as.factor(hiLo) ~., data = revDTM_sentiAfinn_trn %>% select(-review_id),kernel="radial", ranges = list(cost=c(0.1,1,10,50), gamma = c(0.5,1,2,5, 10),scale=FALSE)) )

#Check performance for different tuned parameters
svm_tuneAfinn$performances

#Best model
svm_tuneAfinn$best.parameters
svm_tuneAfinn$best.model

#predictions from best model : Confusion Matrix

revDTM_predTrn_svmAfinn_Best<-predict(svm_tuneAfinn$best.model, revDTM_sentiAfinn_trn)
table(actual= revDTM_sentiAfinn_trn$hiLo, predicted= revDTM_predTrn_svmAfinn_Best)
revDTM_predTst_svmAfinn_best<-predict(svm_tuneAfinn$best.model, revDTM_sentiAfinn_tst)
table(actual= revDTM_sentiAfinn_tst$hiLo, predicted= revDTM_predTst_svmAfinn_best)


#######Combining all the three dictionaries(bing,Nrc,AFINN)

#For combining the matched words of all the three dictionaries change the column 'value' to 'sentiment' in Afinn Dictionary

# try different parameters -- rbf kernel gamma, and cost
system.time( svmM2_Combo <- svm(as.factor(hiLo) ~., data = revDTM_sentiComboDict_trn%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE) )

#Confusion Matrix
revDTM_predTrn_svm2Combo<-predict(svmM2_Combo, revDTM_sentiComboDict_trn)
table(actual= revDTM_sentiComboDict_trn$hiLo, predicted= revDTM_predTrn_svm2Combo)
revDTM_predTst_svm2Combo<-predict(svmM2_Combo, revDTM_sentiComboDict_tst)
table(actual= revDTM_sentiComboDict_tst$hiLo, predicted= revDTM_predTst_svm2Combo)

#use the tune function to do a grid search over a set of parameter values : Parameter Tuning

system.time( svm_tuneCombo <- tune(svm, as.factor(hiLo) ~., data = revDTM_sentiComboDict_trn %>% select(-review_id),kernel="radial", ranges = list(cost=c(0.1,1,10,50), gamma = c(0.5,1,2,5, 10),scale=FALSE)) )

#Check performance for different tuned parameters
svm_tuneCombo$performances

#Best model
svm_tuneCombo$best.parameters
svm_tuneCombo$best.model

#predictions from best model : Confusion Matrix

revDTM_predTrn_svmCombo_Best<-predict(svm_tuneCombo$best.model, revDTM_sentiComboDict_trn)
table(actual= revDTM_sentiComboDict_trn$hiLo, predicted= revDTM_predTrn_svmCombo_Best)
revDTM_predTst_svmACombo_best<-predict(svm_tuneCombo$best.model, revDTM_sentiComboDict_tst)
table(actual= revDTM_sentiComboDict_tst$hiLo, predicted= revDTM_predTst_svm2Combo)

```

#Develop a model on broader set of terms (not just those matching a sentiment dictionary)
```{r message=FALSE, cache=TRUE}

#Remove non alphabetic characters
#rrTokens<-rrTokens %>%  filter(!str_detect(word, "[^[:alpha:]]"))

# reviews in which  each word occur
rWords<-rrTokens %>% group_by(word) %>% summarise(nr=n()) %>% arrange(desc(nr))

#Ungroup rWords
rWords <- ungroup(rWords)

#Dimension of rWords
rWords %>% dim()

#Dimensions for the distinct tokens word in rWords
rWords %>% distinct(word) %>% dim()

#Number of words 
length(rWords$word)

top_n(rWords, 20)
top_n(rWords, -20)

#Words that occur in top 20 reviews
top_words <- top_n(rWords, 20) %>% mutate(word=reorder(word,nr))
ggplot(top_words, aes(word, nr, fill=word)) +geom_col(color="Black")+coord_flip() + scale_y_continuous(name = "Number of Reviews") + scale_x_discrete(name = "Words") + theme(axis.text.y = element_text(hjust = 1, face = "bold", size = 7))

#Words that occur in last 20 reviews
last_words <- top_n(rWords, -20) %>% mutate(word=reorder(word,nr))
ggplot(last_words, aes(word, nr, fill=word)) +geom_col(color="Black")+coord_flip() + scale_y_continuous(name = "Number of Reviews") + scale_x_discrete(name = "Words") + theme(axis.text.y = element_text(hjust = 1, face = "bold", size = 7))


#Remove words which occur in > 90% of reviews, and those which are in, for example, less than 30 reviews
reduced_rWords<-rWords %>% filter(nr< 6000 & nr > 30)
length(reduced_rWords$word)

#reduce the rrTokens data to keep only the reduced set of words
reduced_rrTokens <- left_join(reduced_rWords, rrTokens)

#Dimension of reduced_rrTokens
reduced_rrTokens %>% dim()

#Dimensions for the distinct token words in reduced_rrTokens
reduced_rrTokens %>% distinct(word) %>% dim()

#Now convert it to a DTM, where each row is for a review (document), and columns are the terms (words)
revDTM  <- reduced_rrTokens %>%  pivot_wider(id_cols = c(review_id,starsReview), names_from = word, values_from = tf_idf)  %>% ungroup()

#Check Dimension of revDTM
dim(revDTM)

#Create the dependent variable hiLo of good/bad reviews absed on starsReview, and remove the review with starsReview=3
revDTM <- revDTM %>% filter(starsReview!=3) %>% mutate(hiLo=ifelse(starsReview<=2, -1, 1)) %>% select(-starsReview)

#Dimensions for the distinct token words in revDTM
revDTM %>% distinct(word) %>% dim()

#replace NAs with 0s
revDTM<-revDTM %>% replace(., is.na(.), 0)

#convert hiLo from num to factor
revDTM$hiLo<-as.factor(revDTM$hiLo)

#Number of reviews with 1, -1 class
revDTM %>% group_by(hiLo) %>% tally()

#Create Dataset of 20,000 records

set.seed(1889)
revDTM_10K <- revDTM[sample(nrow(revDTM),20000),]
revDTM <- revDTM_10K

set.seed(1234)

#split the data into training and test dataset (50:50)
revDTM_split<- initial_split(revDTM, 0.5)
revDTM_split_trn  <- training(revDTM_split)
revDTM_split_tst  <- testing(revDTM_split)

#Random Forest Model with number of trees = 100

rfModel1<-ranger(dependent.variable.name = "hiLo", data=revDTM_split_trn %>% select(-review_id), num.trees = 100, importance='permutation', probability = TRUE)

rfModel1

#Make predictions from the model on trn and test dataset
revDTM_split_predTrn<- predict(rfModel1, revDTM_split_trn %>% select(-review_id))
revDTM_split_predTst<- predict(rfModel1, revDTM_split_tst %>% select(-review_id))

#Confusion Matrix at 0.5 for Trn and Tst dataset
table(actual=revDTM_split_trn$hiLo,preds=revDTM_split_predTrn$predictions[,2]>0.5)
table(actual=revDTM_split_tst$hiLo,preds=revDTM_split_predTst$predictions[,2]>0.5)

#Accuracy of Training & test data sets
#mean(predict(rfModel, revDTM_split_trn, type='class') == revDTM_split_predTrn_trn$hiLo)

#table(pred = revSentiAfinn_NBpredTst[,2]>bThr, true=revDTM_sentiNrc_tst$hiLo)
#mean(predict(nbModel3, revDTM_sentiNrc_tst, type='class') == revDTM_sentiNrc_tst$hiLo)

auc(as.numeric(revDTM_sentiNrc_trn$hiLo), revSentiNrc_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiBing_tst$hiLo), revSentiBing_NBpredTst[,2])


#find the optimal TH
rocTrn <- roc(revDTM_split_trn$hiLo,revDTM_split_predTrn$predictions[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_split_tst$hiLo,revDTM_split_predTst$predictions[,2], levels=c(-1, 1))

plot.roc(rocTrn, col='blue', legacy.axes = TRUE)
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#best threshold from ROC
bThr<-coords(rocTrn, "best", ret="threshold", transpose = FALSE)
bThr <- as.numeric(bThr)
bThr

#Confusion Matrix at bThr for Trn and Tst dataset
table(actual=revDTM_split_trn$hiLo,preds=revDTM_split_predTrn$predictions[,2]>bThr)
table(actual=revDTM_split_tst$hiLo,preds=revDTM_split_predTst$predictions[,2]>bThr)


#Random Forest Model with number of trees = 200

rfModel3<-ranger(dependent.variable.name = "hiLo", data=revDTM_split_trn %>% select(-review_id), num.trees = 200, importance='permutation', probability = TRUE)

rfModel3

#Make predictions from the model on trn and test dataset
revDTM_split_predTrn3<- predict(rfModel3, revDTM_split_trn %>% select(-review_id))
revDTM_split_predTst3<- predict(rfModel3, revDTM_split_tst %>% select(-review_id))

#Confusion Matrix at 0.5 for Trn and Tst dataset
table(actual=revDTM_split_trn$hiLo,preds=revDTM_split_predTrn3$predictions[,2]>0.5)
table(actual=revDTM_split_tst$hiLo,preds=revDTM_split_predTst3$predictions[,2]>0.5)

library(pROC)
#find the optimal TH
rocTrn3 <- roc(revDTM_split_trn$hiLo,revDTM_split_predTrn3$predictions[,2], levels=c(-1, 1))
rocTst3 <- roc(revDTM_split_tst$hiLo,revDTM_split_predTst3$predictions[,2], levels=c(-1, 1))

plot.roc(rocTrn3, col='blue', legacy.axes = TRUE)
plot.roc(rocTst3, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

#best threshold from ROC
bThr3 <-coords(rocTrn3, "best", ret="threshold", transpose = FALSE)
bThr3 <- as.numeric(bThr3)
bThr3

#Confusion Matrix at bThr for Trn and Tst dataset - Random Forest
table(actual=revDTM_split_trn$hiLo,preds=revDTM_split_predTrn3$predictions[,2]>bThr)
table(actual=revDTM_split_tst$hiLo,preds=revDTM_split_predTst3$predictions[,2]>bThr)

#Naive Bayes Model without smoothing
NaiveBayesModel1 <- naiveBayes(hiLo ~ ., data=revDTM_split_trn %>% select(-review_id))

#Make predictions from the NB model on trn and test dataset
revDTM_predTrnNB<- predict(NaiveBayesModel1, revDTM_split_trn %>% select(-review_id))
revDTM_predTstNB<- predict(NaiveBayesModel1, revDTM_split_tst %>% select(-review_id))

#Confusion Matrix at 0.5 for Trn and Tst dataset - Naive Bayes without smoothing
a1 <- table(actual=revDTM_split_trn$hiLo,preds=revDTM_predTrnNB)
b1 <- table(actual=revDTM_split_tst$hiLo,preds=revDTM_predTstNB)

# For Training data set
confusionMatrix(a1)

# For Test data set
confusionMatrix(a1)

#Naive Bayes Model with smoothing
NaiveBayesModel1 <- naiveBayes(hiLo ~ ., data=revDTM_split_trn %>% select(-review_id), laplace = 1)

#Make predictions from the NB model on trn and test dataset
revDTM_predTrnNB<- predict(NaiveBayesModel1, revDTM_split_trn %>% select(-review_id))
revDTM_predTstNB<- predict(NaiveBayesModel1, revDTM_split_tst %>% select(-review_id))

#Confusion Matrix at 0.5 for Trn and Tst dataset - Naive bayes with Laplace
a1 <- table(actual=revDTM_split_trn$hiLo,preds=revDTM_predTrnNB)
b1 <- table(actual=revDTM_split_tst$hiLo,preds=revDTM_predTstNB)

# For Training data set
confusionMatrix(a1)

# For Test data set
confusionMatrix(a1)

#Support Vector Machine Model

system.time( svm_BT <- svm(as.factor(hiLo) ~., data = revDTM_split_trn%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE) )

#predictions from best model : Confusion Matrix

revDTM_predTrn_svmBT<-predict(svm_BT, revDTM_split_trn)
table(actual= revDTM_split_trn$hiLo, predicted= revDTM_predTrn_svmBT)
revDTM_predTst_svmBT<-predict(svm_BT, revDTM_split_tst)
table(actual= revDTM_split_tst$hiLo, predicted= revDTM_predTst_svmBT)

```

